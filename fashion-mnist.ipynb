{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.复现波士顿房价预测代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_label), (test_data, test_label) = tf.keras.datasets.boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建神经网络模型\n",
    "### 样本有13个特征，用13个输入节点，类别标号用1个神经元\n",
    "### 对应输入层13个节点，输出层1个神经元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0, train loss 186.4791, val loss 462.6805\n",
      "epoch   1, train loss 185.6122, val loss 173.4911\n",
      "epoch   2, train loss 107.5209, val loss 133.9700\n",
      "epoch   3, train loss 112.8902, val loss 110.4484\n",
      "epoch   4, train loss 108.6389, val loss 99.5674\n",
      "epoch   5, train loss 111.9629, val loss 95.3996\n",
      "epoch   6, train loss 78.0696, val loss 93.3494\n",
      "epoch   7, train loss 149.4837, val loss 95.0955\n",
      "epoch   8, train loss 85.5454, val loss 93.0854\n",
      "epoch   9, train loss 88.5140, val loss 95.4506\n",
      "epoch  10, train loss 74.8061, val loss 91.3358\n",
      "epoch  11, train loss 70.3077, val loss 91.9780\n",
      "epoch  12, train loss 19.3951, val loss 91.6357\n",
      "epoch  13, train loss 81.7275, val loss 90.9258\n",
      "epoch  14, train loss 101.5048, val loss 93.0668\n",
      "epoch  15, train loss 104.1625, val loss 91.7508\n",
      "epoch  16, train loss 136.1601, val loss 91.5094\n",
      "epoch  17, train loss 99.2767, val loss 89.1939\n",
      "epoch  18, train loss 58.3621, val loss 89.3839\n",
      "epoch  19, train loss 100.1761, val loss 94.5343\n",
      "epoch  20, train loss 97.0196, val loss 90.0335\n",
      "epoch  21, train loss 70.2885, val loss 85.4469\n",
      "epoch  22, train loss 99.8478, val loss 90.0945\n",
      "epoch  23, train loss 108.7317, val loss 91.0631\n",
      "epoch  24, train loss 97.5259, val loss 89.9868\n",
      "epoch  25, train loss 71.0070, val loss 93.4437\n",
      "epoch  26, train loss 99.2235, val loss 85.2500\n",
      "epoch  27, train loss 119.2405, val loss 88.6942\n",
      "epoch  28, train loss 113.5874, val loss 90.7481\n",
      "epoch  29, train loss 111.0890, val loss 88.4568\n",
      "epoch  30, train loss 74.1370, val loss 86.7012\n",
      "epoch  31, train loss 107.1400, val loss 90.9054\n",
      "epoch  32, train loss 56.8245, val loss 90.7349\n",
      "epoch  33, train loss 87.8495, val loss 91.8673\n",
      "epoch  34, train loss 71.0604, val loss 89.8445\n",
      "epoch  35, train loss 66.5778, val loss 90.1547\n",
      "epoch  36, train loss 66.2634, val loss 90.4078\n",
      "epoch  37, train loss 110.0056, val loss 86.7146\n",
      "epoch  38, train loss 95.9696, val loss 89.8614\n",
      "epoch  39, train loss 109.1150, val loss 87.5988\n",
      "epoch  40, train loss 92.9557, val loss 87.1366\n",
      "epoch  41, train loss 71.6010, val loss 90.1857\n",
      "epoch  42, train loss 118.1694, val loss 85.6599\n",
      "epoch  43, train loss 93.4835, val loss 86.1087\n",
      "epoch  44, train loss 93.2649, val loss 90.9470\n",
      "epoch  45, train loss 115.4076, val loss 87.9405\n",
      "epoch  46, train loss 39.6811, val loss 90.3157\n",
      "epoch  47, train loss 75.6233, val loss 91.3404\n",
      "epoch  48, train loss 68.6793, val loss 91.1466\n",
      "epoch  49, train loss 115.3873, val loss 90.3752\n",
      "epoch  50, train loss 91.2476, val loss 83.7091\n",
      "epoch  51, train loss 105.7368, val loss 89.2393\n",
      "epoch  52, train loss 65.8603, val loss 87.5392\n",
      "epoch  53, train loss 81.4985, val loss 89.8503\n",
      "epoch  54, train loss 118.1821, val loss 81.7617\n",
      "epoch  55, train loss 52.4824, val loss 88.3370\n",
      "epoch  56, train loss 120.4911, val loss 90.8216\n",
      "epoch  57, train loss 77.8491, val loss 89.3075\n",
      "epoch  58, train loss 93.3888, val loss 88.6551\n",
      "epoch  59, train loss 71.4332, val loss 89.9798\n",
      "epoch  60, train loss 101.2161, val loss 90.2752\n",
      "epoch  61, train loss 97.8883, val loss 89.5602\n",
      "epoch  62, train loss 85.5550, val loss 91.3543\n",
      "epoch  63, train loss 160.3816, val loss 88.8916\n",
      "epoch  64, train loss 84.1714, val loss 89.6660\n",
      "epoch  65, train loss 77.5834, val loss 91.5449\n",
      "epoch  66, train loss 43.2472, val loss 87.9988\n",
      "epoch  67, train loss 59.7414, val loss 87.2875\n",
      "epoch  68, train loss 125.9502, val loss 89.0689\n",
      "epoch  69, train loss 86.5176, val loss 87.5496\n",
      "epoch  70, train loss 70.3179, val loss 89.3319\n",
      "epoch  71, train loss 104.6301, val loss 88.3885\n",
      "epoch  72, train loss 67.9404, val loss 88.8539\n",
      "epoch  73, train loss 56.1407, val loss 90.4079\n",
      "epoch  74, train loss 60.0226, val loss 86.7053\n",
      "epoch  75, train loss 77.4369, val loss 88.5513\n",
      "epoch  76, train loss 51.6867, val loss 89.1668\n",
      "epoch  77, train loss 81.4624, val loss 88.3917\n",
      "epoch  78, train loss 99.5080, val loss 89.6102\n",
      "epoch  79, train loss 117.4361, val loss 90.0258\n",
      "epoch  80, train loss 90.8620, val loss 90.4093\n",
      "epoch  81, train loss 54.4111, val loss 88.8704\n",
      "epoch  82, train loss 135.0541, val loss 89.3070\n",
      "epoch  83, train loss 39.4836, val loss 83.9820\n",
      "epoch  84, train loss 85.5625, val loss 90.3504\n",
      "epoch  85, train loss 51.0298, val loss 88.5039\n",
      "epoch  86, train loss 50.4596, val loss 87.4687\n",
      "epoch  87, train loss 89.0964, val loss 86.3545\n",
      "epoch  88, train loss 47.4981, val loss 90.6229\n",
      "epoch  89, train loss 59.1229, val loss 87.3342\n",
      "epoch  90, train loss 138.2839, val loss 86.5368\n",
      "epoch  91, train loss 117.5949, val loss 88.7733\n",
      "epoch  92, train loss 68.7214, val loss 90.9378\n",
      "epoch  93, train loss 78.7563, val loss 88.1092\n",
      "epoch  94, train loss 62.8375, val loss 85.5741\n",
      "epoch  95, train loss 114.9032, val loss 90.2743\n",
      "epoch  96, train loss 63.2823, val loss 89.3440\n",
      "epoch  97, train loss 78.0462, val loss 84.2305\n",
      "epoch  98, train loss 136.4727, val loss 90.5490\n",
      "epoch  99, train loss 81.6392, val loss 89.1010\n",
      "epoch 100, train loss 75.6463, val loss 88.1790\n",
      "epoch 101, train loss 124.3413, val loss 89.3812\n",
      "epoch 102, train loss 146.1067, val loss 85.9778\n",
      "epoch 103, train loss 92.5421, val loss 90.0926\n",
      "epoch 104, train loss 107.0940, val loss 87.7647\n",
      "epoch 105, train loss 112.7102, val loss 87.4232\n",
      "epoch 106, train loss 96.8713, val loss 87.4337\n",
      "epoch 107, train loss 96.9733, val loss 86.9200\n",
      "epoch 108, train loss 70.5003, val loss 84.8323\n",
      "epoch 109, train loss 105.7086, val loss 87.1802\n",
      "epoch 110, train loss 103.0926, val loss 86.9450\n",
      "epoch 111, train loss 152.0188, val loss 90.1628\n",
      "epoch 112, train loss 71.4402, val loss 88.9887\n",
      "epoch 113, train loss 122.2689, val loss 89.3081\n",
      "epoch 114, train loss 110.4136, val loss 88.5106\n",
      "epoch 115, train loss 80.7844, val loss 86.2008\n",
      "epoch 116, train loss 85.2953, val loss 86.4561\n",
      "epoch 117, train loss 77.6443, val loss 88.9795\n",
      "epoch 118, train loss 72.0722, val loss 88.9043\n",
      "epoch 119, train loss 102.5117, val loss 90.3601\n",
      "epoch 120, train loss 65.4273, val loss 88.7943\n",
      "epoch 121, train loss 107.8383, val loss 86.5220\n",
      "epoch 122, train loss 107.2670, val loss 90.1007\n",
      "epoch 123, train loss 86.7545, val loss 86.2709\n",
      "epoch 124, train loss 67.4483, val loss 86.1508\n",
      "epoch 125, train loss 69.4271, val loss 89.4168\n",
      "epoch 126, train loss 73.1861, val loss 87.4555\n",
      "epoch 127, train loss 76.1068, val loss 88.3594\n",
      "epoch 128, train loss 68.7848, val loss 89.8758\n",
      "epoch 129, train loss 127.8290, val loss 88.0316\n",
      "epoch 130, train loss 88.1629, val loss 89.3104\n",
      "epoch 131, train loss 37.5465, val loss 88.0805\n",
      "epoch 132, train loss 110.1437, val loss 83.5295\n",
      "epoch 133, train loss 87.9511, val loss 88.6929\n",
      "epoch 134, train loss 103.5772, val loss 85.2570\n",
      "epoch 135, train loss 77.8736, val loss 90.2507\n",
      "epoch 136, train loss 96.8396, val loss 89.2554\n",
      "epoch 137, train loss 81.4309, val loss 89.7851\n",
      "epoch 138, train loss 83.8074, val loss 89.2892\n",
      "epoch 139, train loss 101.3961, val loss 88.7374\n",
      "epoch 140, train loss 61.2189, val loss 90.8815\n",
      "epoch 141, train loss 125.5838, val loss 82.1381\n",
      "epoch 142, train loss 102.2882, val loss 91.1256\n",
      "epoch 143, train loss 121.6891, val loss 90.1477\n",
      "epoch 144, train loss 106.3228, val loss 88.1224\n",
      "epoch 145, train loss 113.3072, val loss 87.1645\n",
      "epoch 146, train loss 93.7014, val loss 88.7816\n",
      "epoch 147, train loss 129.9128, val loss 90.6695\n",
      "epoch 148, train loss 86.1757, val loss 89.8097\n",
      "epoch 149, train loss 58.8159, val loss 86.1891\n",
      "epoch 150, train loss 64.4425, val loss 84.5958\n",
      "epoch 151, train loss 113.8954, val loss 87.3613\n",
      "epoch 152, train loss 63.9421, val loss 86.0038\n",
      "epoch 153, train loss 33.1893, val loss 86.5442\n",
      "epoch 154, train loss 141.4384, val loss 90.4313\n",
      "epoch 155, train loss 69.8734, val loss 88.1845\n",
      "epoch 156, train loss 119.5330, val loss 87.9560\n",
      "epoch 157, train loss 136.5812, val loss 85.5186\n",
      "epoch 158, train loss 69.3638, val loss 85.8976\n",
      "epoch 159, train loss 54.4689, val loss 90.3757\n",
      "epoch 160, train loss 88.3329, val loss 84.2275\n",
      "epoch 161, train loss 64.6233, val loss 89.1124\n",
      "epoch 162, train loss 101.4700, val loss 89.0233\n",
      "epoch 163, train loss 66.8470, val loss 87.2822\n",
      "epoch 164, train loss 93.5921, val loss 90.7978\n",
      "epoch 165, train loss 90.4430, val loss 88.5638\n",
      "epoch 166, train loss 94.5636, val loss 90.8639\n",
      "epoch 167, train loss 92.3346, val loss 85.8316\n",
      "epoch 168, train loss 39.6631, val loss 90.5854\n",
      "epoch 169, train loss 135.8716, val loss 90.5681\n",
      "epoch 170, train loss 53.3599, val loss 87.2543\n",
      "epoch 171, train loss 80.3812, val loss 87.5714\n",
      "epoch 172, train loss 73.4769, val loss 89.5853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 173, train loss 63.1365, val loss 86.2493\n",
      "epoch 174, train loss 108.2589, val loss 90.4931\n",
      "epoch 175, train loss 101.0409, val loss 86.9532\n",
      "epoch 176, train loss 50.7456, val loss 86.8341\n",
      "epoch 177, train loss 133.0567, val loss 89.0229\n",
      "epoch 178, train loss 60.3068, val loss 86.5886\n",
      "epoch 179, train loss 89.3309, val loss 90.3397\n",
      "epoch 180, train loss 66.7589, val loss 88.1717\n",
      "epoch 181, train loss 144.8993, val loss 89.7373\n",
      "epoch 182, train loss 98.9004, val loss 90.0569\n",
      "epoch 183, train loss 138.7227, val loss 85.5335\n",
      "epoch 184, train loss 134.9633, val loss 87.4311\n",
      "epoch 185, train loss 73.6953, val loss 90.2550\n",
      "epoch 186, train loss 35.1896, val loss 87.2517\n",
      "epoch 187, train loss 80.8249, val loss 86.4983\n",
      "epoch 188, train loss 87.3772, val loss 88.7725\n",
      "epoch 189, train loss 91.6497, val loss 86.0316\n",
      "epoch 190, train loss 101.7258, val loss 87.8313\n",
      "epoch 191, train loss 105.3041, val loss 88.3333\n",
      "epoch 192, train loss 118.0810, val loss 89.4107\n",
      "epoch 193, train loss 95.8465, val loss 90.2746\n",
      "epoch 194, train loss 95.0290, val loss 90.2735\n",
      "epoch 195, train loss 125.8788, val loss 87.7890\n",
      "epoch 196, train loss 79.4060, val loss 90.0019\n",
      "epoch 197, train loss 71.0441, val loss 91.8878\n",
      "epoch 198, train loss 87.8441, val loss 88.1923\n",
      "epoch 199, train loss 80.0098, val loss 89.0275\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    inputs = tf.placeholder(shape=[None, 13], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "    \n",
    "    h1 = tf.keras.layers.Dense(64, activation=tf.nn.relu)(inputs)\n",
    "    h2 = tf.keras.layers.Dense(16, activation=tf.nn.relu)(h1)\n",
    "    output = tf.keras.layers.Dense(1, activation=None)(h2)\n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "        tf.keras.losses.mean_squared_error(labels, output))\n",
    "    \n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=1e-6)\n",
    "    train_op = optim.minimize(loss)\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batch_size= 32\n",
    "    for epoch in range(200):\n",
    "        order = np.argsort(np.random.random(train_label.shape))\n",
    "        train_data = train_data[order]\n",
    "        train_label = train_label[order]\n",
    "        \n",
    "        res_train_losses = []\n",
    "        for i in range(train_label.shape[0] // batch_size):\n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size\n",
    "            res_train_loss, _ = sess.run([loss, train_op],\n",
    "                     feed_dict={inputs: train_data[start: end],\n",
    "                                labels: train_label[start: end]})\n",
    "            res_train_losses.append(res_train_loss)\n",
    "        res_val_loss = sess.run(loss,\n",
    "                 feed_dict={inputs: test_data,\n",
    "                            labels: test_label})\n",
    "        print('epoch %3d, train loss %2.4f, val loss %2.4f' %\n",
    "              (epoch, res_train_loss, np.mean(res_train_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.为上述模型训练添加代价变化的TensorBoard摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.导入Fashion_MNIST数据集并可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 划分训练集和测试集（60000:10000）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_label), (test_data, test_label) = fashion_mnist\n",
    "train_label = tf.one_hot(train_label, depth=10)\n",
    "test_label = tf.one_hot(test_label,depth=10)\n",
    "#number = {'0':0,'1':1,'2':2,'3':3,'4':4,'5':5,'6':6,'7':7,'8':8,'9':9}\n",
    "#for i in range(len(train_label)):\n",
    "#    train_label = to_categorical(number[train_label[i]],10)\n",
    "#print(train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseSession.close of <tensorflow.python.client.session.Session object at 0x7f4061607a58>>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess1 = tf.Session()\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "train_label = train_label.eval(session=sess1)\n",
    "test_label = test_label.eval(session=sess1)\n",
    "sess1.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可视化第一张图片(28*28个特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4061427ba8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Image.fromarray(train_data[0].reshape([28,28])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4,5.构建全连接神经网络并使用导入的数据训练分类器并改进神经网络的结构与训练方法提高模型性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "train_data.resize(60000,28*28)\n",
    "test_data.resize(10000,28*28)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875\n",
      "312\n"
     ]
    }
   ],
   "source": [
    "#group = train_data.shape[0]//32\n",
    "#print(group)\n",
    "#group_test = test_data.shape[0]//32\n",
    "#print(group_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 2.2208, acc 0.1246\n",
      "step     1, loss 2.0347, acc 0.2074\n",
      "step     2, loss 1.7912, acc 0.2736\n",
      "step     3, loss 1.6899, acc 0.3474\n",
      "step     4, loss 1.6594, acc 0.3608\n",
      "step     5, loss 1.6360, acc 0.3665\n",
      "step     6, loss 1.6188, acc 0.3701\n",
      "step     7, loss 1.6065, acc 0.3724\n",
      "step     8, loss 1.5940, acc 0.3745\n",
      "step     9, loss 1.5110, acc 0.4031\n",
      "step    10, loss 1.4967, acc 0.4615\n",
      "step    11, loss 1.4741, acc 0.4657\n",
      "step    12, loss 1.4594, acc 0.4660\n",
      "step    13, loss 1.4461, acc 0.4669\n",
      "step    14, loss 1.4269, acc 0.4688\n",
      "step    15, loss 1.4232, acc 0.4701\n",
      "step    16, loss 1.4153, acc 0.4713\n",
      "step    17, loss 1.4072, acc 0.4712\n",
      "step    18, loss 1.3996, acc 0.4719\n",
      "step    19, loss 1.3951, acc 0.4725\n",
      "step    20, loss 1.3817, acc 0.4727\n",
      "step    21, loss 1.3748, acc 0.4732\n",
      "step    22, loss 1.3770, acc 0.4737\n",
      "step    23, loss 1.3709, acc 0.4740\n",
      "step    24, loss 1.3589, acc 0.4741\n",
      "step    25, loss 1.3547, acc 0.4742\n",
      "step    26, loss 1.3509, acc 0.4748\n",
      "step    27, loss 1.3470, acc 0.4749\n",
      "step    28, loss 1.3451, acc 0.4750\n",
      "step    29, loss 1.3418, acc 0.4751\n",
      "step    30, loss 1.3423, acc 0.4752\n",
      "step    31, loss 1.3421, acc 0.4753\n",
      "step    32, loss 1.3423, acc 0.4756\n",
      "step    33, loss 1.3408, acc 0.4755\n",
      "step    34, loss 1.3278, acc 0.4769\n",
      "step    35, loss 1.3289, acc 0.4768\n",
      "step    36, loss 1.3260, acc 0.4797\n",
      "step    37, loss 1.2824, acc 0.4911\n",
      "step    38, loss 1.2154, acc 0.5186\n",
      "step    39, loss 1.1255, acc 0.5642\n",
      "step    40, loss 1.1102, acc 0.5715\n",
      "step    41, loss 1.0964, acc 0.5713\n",
      "step    42, loss 1.0892, acc 0.5722\n",
      "step    43, loss 1.0790, acc 0.5751\n",
      "step    44, loss 1.0695, acc 0.5753\n",
      "step    45, loss 1.0561, acc 0.5742\n",
      "step    46, loss 0.9674, acc 0.5897\n",
      "step    47, loss 0.9238, acc 0.6254\n",
      "step    48, loss 0.9109, acc 0.6435\n",
      "step    49, loss 0.8983, acc 0.6514\n",
      "step    50, loss 0.8816, acc 0.6626\n",
      "step    51, loss 0.7966, acc 0.6817\n",
      "step    52, loss 0.7986, acc 0.7467\n",
      "step    53, loss 0.5699, acc 0.7960\n",
      "step    54, loss 0.5586, acc 0.8096\n",
      "step    55, loss 0.5613, acc 0.8167\n",
      "step    56, loss 0.5695, acc 0.8209\n",
      "step    57, loss 0.5429, acc 0.8251\n",
      "step    58, loss 0.4831, acc 0.8283\n",
      "step    59, loss 0.4608, acc 0.8302\n",
      "step    60, loss 0.4530, acc 0.8315\n",
      "step    61, loss 0.4176, acc 0.8327\n",
      "step    62, loss 0.4255, acc 0.8352\n",
      "step    63, loss 0.4205, acc 0.8356\n",
      "step    64, loss 0.4179, acc 0.8367\n",
      "step    65, loss 0.4158, acc 0.8382\n",
      "step    66, loss 0.4075, acc 0.8394\n",
      "step    67, loss 0.3965, acc 0.8397\n",
      "step    68, loss 0.4263, acc 0.8407\n",
      "step    69, loss 0.4025, acc 0.8414\n",
      "step    70, loss 0.4136, acc 0.8417\n",
      "step    71, loss 0.4178, acc 0.8429\n",
      "step    72, loss 0.4051, acc 0.8435\n",
      "step    73, loss 0.4073, acc 0.8438\n",
      "step    74, loss 0.3956, acc 0.8447\n",
      "step    75, loss 0.3861, acc 0.8446\n",
      "step    76, loss 0.3861, acc 0.8453\n",
      "step    77, loss 0.3740, acc 0.8457\n",
      "step    78, loss 0.3697, acc 0.8460\n",
      "step    79, loss 0.3697, acc 0.8466\n",
      "step    80, loss 0.3705, acc 0.8469\n",
      "step    81, loss 0.3504, acc 0.8468\n",
      "step    82, loss 0.3663, acc 0.8471\n",
      "step    83, loss 0.3604, acc 0.8472\n",
      "step    84, loss 0.3469, acc 0.8476\n",
      "step    85, loss 0.3562, acc 0.8479\n",
      "step    86, loss 0.3417, acc 0.8484\n",
      "step    87, loss 0.3625, acc 0.8486\n",
      "step    88, loss 0.3531, acc 0.8487\n",
      "step    89, loss 0.3362, acc 0.8486\n",
      "step    90, loss 0.3211, acc 0.8489\n",
      "step    91, loss 0.3304, acc 0.8489\n",
      "step    92, loss 0.3388, acc 0.8490\n",
      "step    93, loss 0.3208, acc 0.8497\n",
      "step    94, loss 0.3211, acc 0.8497\n",
      "step    95, loss 0.3013, acc 0.8499\n",
      "step    96, loss 0.3062, acc 0.8496\n",
      "step    97, loss 0.3107, acc 0.8501\n",
      "step    98, loss 0.3083, acc 0.8503\n",
      "step    99, loss 0.3018, acc 0.8505\n",
      "step   100, loss 0.2965, acc 0.8509\n",
      "step   101, loss 0.2857, acc 0.8509\n",
      "step   102, loss 0.2997, acc 0.8511\n",
      "step   103, loss 0.3150, acc 0.8513\n",
      "step   104, loss 0.3043, acc 0.8513\n",
      "step   105, loss 0.2851, acc 0.8516\n",
      "step   106, loss 0.2973, acc 0.8510\n",
      "step   107, loss 0.2923, acc 0.8518\n",
      "step   108, loss 0.2915, acc 0.8521\n",
      "step   109, loss 0.2906, acc 0.8519\n",
      "step   110, loss 0.2960, acc 0.8518\n",
      "step   111, loss 0.2963, acc 0.8518\n",
      "step   112, loss 0.2910, acc 0.8523\n",
      "step   113, loss 0.2911, acc 0.8525\n",
      "step   114, loss 0.2782, acc 0.8521\n",
      "step   115, loss 0.2860, acc 0.8526\n",
      "step   116, loss 0.2781, acc 0.8523\n",
      "step   117, loss 0.2817, acc 0.8526\n",
      "step   118, loss 0.3028, acc 0.8527\n",
      "step   119, loss 0.2819, acc 0.8524\n",
      "step   120, loss 0.2880, acc 0.8529\n",
      "step   121, loss 0.2734, acc 0.8531\n",
      "step   122, loss 0.2766, acc 0.8527\n",
      "step   123, loss 0.2664, acc 0.8530\n",
      "step   124, loss 0.2716, acc 0.8533\n",
      "step   125, loss 0.2815, acc 0.8534\n",
      "step   126, loss 0.2422, acc 0.8531\n",
      "step   127, loss 0.2486, acc 0.8531\n",
      "step   128, loss 0.2852, acc 0.8525\n",
      "step   129, loss 0.2719, acc 0.8529\n",
      "step   130, loss 0.2276, acc 0.8531\n",
      "step   131, loss 0.2590, acc 0.8542\n",
      "step   132, loss 0.2644, acc 0.8535\n",
      "step   133, loss 0.2297, acc 0.8542\n",
      "step   134, loss 0.2244, acc 0.8536\n",
      "step   135, loss 0.2162, acc 0.8541\n",
      "step   136, loss 0.2288, acc 0.8537\n",
      "step   137, loss 0.2045, acc 0.8538\n",
      "step   138, loss 0.2030, acc 0.8537\n",
      "step   139, loss 0.2191, acc 0.8534\n",
      "step   140, loss 0.2096, acc 0.8536\n",
      "step   141, loss 0.2034, acc 0.8539\n",
      "step   142, loss 0.2057, acc 0.8540\n",
      "step   143, loss 0.2344, acc 0.8533\n",
      "step   144, loss 0.2016, acc 0.8535\n",
      "step   145, loss 0.2109, acc 0.8530\n",
      "step   146, loss 0.2011, acc 0.8540\n",
      "step   147, loss 0.2208, acc 0.8539\n",
      "step   148, loss 0.3221, acc 0.8527\n",
      "step   149, loss 0.1955, acc 0.8539\n",
      "step   150, loss 0.1907, acc 0.8531\n",
      "step   151, loss 0.1957, acc 0.8534\n",
      "step   152, loss 0.2121, acc 0.8541\n",
      "step   153, loss 0.1946, acc 0.8538\n",
      "step   154, loss 0.1903, acc 0.8538\n",
      "step   155, loss 0.1846, acc 0.8541\n",
      "step   156, loss 0.1860, acc 0.8541\n",
      "step   157, loss 0.1988, acc 0.8539\n",
      "step   158, loss 0.1927, acc 0.8543\n",
      "step   159, loss 0.1973, acc 0.8541\n",
      "step   160, loss 0.1859, acc 0.8539\n",
      "step   161, loss 0.1990, acc 0.8543\n",
      "step   162, loss 0.3066, acc 0.8524\n",
      "step   163, loss 0.1963, acc 0.8533\n",
      "step   164, loss 0.2036, acc 0.8544\n",
      "step   165, loss 0.1985, acc 0.8537\n",
      "step   166, loss 0.1903, acc 0.8539\n",
      "step   167, loss 0.2510, acc 0.8541\n",
      "step   168, loss 0.3006, acc 0.8542\n",
      "step   169, loss 0.1800, acc 0.8546\n",
      "step   170, loss 0.2076, acc 0.8539\n",
      "step   171, loss 0.2046, acc 0.8547\n",
      "step   172, loss 0.1986, acc 0.8540\n",
      "step   173, loss 0.2990, acc 0.8547\n",
      "step   174, loss 0.1949, acc 0.8546\n",
      "step   175, loss 0.2609, acc 0.8538\n",
      "step   176, loss 0.1922, acc 0.8543\n",
      "step   177, loss 0.1918, acc 0.8549\n",
      "step   178, loss 0.1932, acc 0.8542\n",
      "step   179, loss 0.3569, acc 0.8546\n",
      "step   180, loss 0.2035, acc 0.8539\n",
      "step   181, loss 0.1916, acc 0.8531\n",
      "step   182, loss 0.3684, acc 0.8527\n",
      "step   183, loss 0.1999, acc 0.8534\n",
      "step   184, loss 0.1881, acc 0.8535\n",
      "step   185, loss 0.1835, acc 0.8537\n",
      "step   186, loss 0.1712, acc 0.8542\n",
      "step   187, loss 0.1693, acc 0.8546\n",
      "step   188, loss 0.1726, acc 0.8547\n",
      "step   189, loss 0.1809, acc 0.8549\n",
      "step   190, loss 0.2328, acc 0.8544\n",
      "step   191, loss 0.1712, acc 0.8545\n",
      "step   192, loss 0.1824, acc 0.8533\n",
      "step   193, loss 0.2207, acc 0.8539\n",
      "step   194, loss 0.1640, acc 0.8537\n",
      "step   195, loss 0.1765, acc 0.8544\n",
      "step   196, loss 0.1703, acc 0.8545\n",
      "step   197, loss 0.1827, acc 0.8542\n",
      "step   198, loss 0.1568, acc 0.8542\n",
      "step   199, loss 0.3499, acc 0.8532\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    inputs = tf.placeholder(shape=[None,784],dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None,10],dtype=tf.float32)\n",
    "    \n",
    "    #output = tf.keras.layers.Dense(1,activation=None)(inputs)\n",
    "    #h1 = tf.keras.layers.Dense(64, activation=tf.nn.relu)(inputs)\n",
    "    h1 = tf.keras.layers.Dense(64, activation=tf.nn.relu)(inputs)\n",
    "    h2 = tf.keras.layers.Dense(32,activation=tf.nn.relu)(h1)\n",
    "    h3 = tf.keras.layers.Dense(16,activation=tf.nn.relu)(h2)\n",
    "    output = tf.keras.layers.Dense(10, activation=None)(h3)\n",
    "    \n",
    "    #代价函数\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=labels))\n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels,axis=1), tf.argmax(output,axis=1)),\n",
    "                tf.float32))\n",
    "    \n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optim.minimize(loss)\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batch_size = 32\n",
    "    \n",
    "    #训练模型\n",
    "    group_train = train_data.shape[0]//32\n",
    "    group_test = test_data.shape[0]//32\n",
    "    for step in range(200):\n",
    "        acc_s = []\n",
    "        for train_step in range(group_train):\n",
    "            batch_images = train_data[train_step*32:train_step*32+32,:]#shape(32,784)\n",
    "            batch_labels = train_label[train_step*32:train_step*32+32,:]#shape(32)\n",
    "            res_loss,_ = sess.run([loss,train_op],feed_dict={inputs:batch_images,labels:batch_labels})\n",
    "            \n",
    "            #输出代价并验证模型\n",
    "            if train_step % 100 == 0:\n",
    "                accs = []\n",
    "                for test_step in range(group_test):\n",
    "                    batch_test_images = test_data[test_step*32:test_step*32+32,:]\n",
    "                    batch_test_labels = test_label[test_step*32:test_step*32+32,:]\n",
    "                    res_acc = sess.run(acc, feed_dict={inputs: batch_test_images,labels: batch_test_labels})\n",
    "                    accs.append(res_acc)\n",
    "                accs = np.mean(accs)\n",
    "                acc_s.append(accs)\n",
    "        acc_s = np.mean(acc_s)\n",
    "        print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, acc_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.复现上述使用高级API实现的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-8086832a2c95>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/liuzixuan/test/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/liuzixuan/test/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /home/liuzixuan/机器学习/深度/小练习/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/liuzixuan/test/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /home/liuzixuan/机器学习/深度/小练习/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/liuzixuan/test/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /home/liuzixuan/机器学习/深度/小练习/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/liuzixuan/机器学习/深度/小练习/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/liuzixuan/test/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# 下载MNIST数据集并生成DataSet对象\n",
    "# 使用OneHot编码处理标记\n",
    "mnist = input_data.read_data_sets(\"/home/liuzixuan/机器学习/深度/小练习/MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 114.3885, acc 0.1179\n",
      "step   500, loss 5.1310, acc 0.7533\n",
      "step  1000, loss 4.0867, acc 0.8137\n",
      "step  1500, loss 3.3145, acc 0.8360\n",
      "step  2000, loss 0.0475, acc 0.8466\n",
      "step  2500, loss 2.2935, acc 0.8569\n",
      "step  3000, loss 3.2364, acc 0.8637\n",
      "step  3500, loss 2.2068, acc 0.8654\n",
      "step  4000, loss 2.5488, acc 0.8736\n",
      "step  4500, loss 1.2280, acc 0.8776\n",
      "step  5000, loss 0.3639, acc 0.8803\n",
      "step  5500, loss 1.6963, acc 0.8831\n",
      "step  6000, loss 0.7630, acc 0.8834\n",
      "step  6500, loss 1.2554, acc 0.8839\n",
      "step  7000, loss 2.3994, acc 0.8856\n",
      "step  7500, loss 0.3354, acc 0.8853\n",
      "step  8000, loss 1.1181, acc 0.8884\n",
      "step  8500, loss 0.8470, acc 0.8893\n",
      "step  9000, loss 0.4253, acc 0.8909\n",
      "step  9500, loss 1.4272, acc 0.8918\n",
      "step 10000, loss 0.4894, acc 0.8921\n",
      "step 10500, loss 1.4548, acc 0.8931\n",
      "step 11000, loss 2.1723, acc 0.8933\n",
      "step 11500, loss 0.0012, acc 0.8957\n",
      "step 12000, loss 0.1002, acc 0.8950\n",
      "step 12500, loss 0.9123, acc 0.8949\n",
      "step 13000, loss 1.1525, acc 0.8963\n",
      "step 13500, loss 0.5771, acc 0.8926\n",
      "step 14000, loss 0.3969, acc 0.8960\n",
      "step 14500, loss 0.7976, acc 0.8985\n",
      "step 15000, loss 0.4719, acc 0.8954\n",
      "step 15500, loss 0.1101, acc 0.8971\n",
      "step 16000, loss 0.1836, acc 0.8981\n",
      "step 16500, loss 1.3584, acc 0.8979\n",
      "step 17000, loss 0.0242, acc 0.9007\n",
      "step 17500, loss 1.0727, acc 0.9000\n",
      "step 18000, loss 0.1907, acc 0.9018\n",
      "step 18500, loss 1.3710, acc 0.8971\n",
      "step 19000, loss 1.6377, acc 0.9034\n",
      "step 19500, loss 0.0683, acc 0.9009\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.relu(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            logits=logits, labels=labels))\n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 2.3587, acc 0.0898\n",
      "step   500, loss 0.6398, acc 0.8546\n",
      "step  1000, loss 0.3573, acc 0.8879\n",
      "step  1500, loss 0.5553, acc 0.8991\n",
      "step  2000, loss 0.3808, acc 0.9062\n",
      "step  2500, loss 0.3386, acc 0.9083\n",
      "step  3000, loss 0.4919, acc 0.9132\n",
      "step  3500, loss 0.6852, acc 0.9178\n",
      "step  4000, loss 0.2629, acc 0.9204\n",
      "step  4500, loss 0.3801, acc 0.9204\n",
      "step  5000, loss 0.1363, acc 0.9249\n",
      "step  5500, loss 0.0648, acc 0.9241\n",
      "step  6000, loss 0.0802, acc 0.9266\n",
      "step  6500, loss 0.5294, acc 0.9287\n",
      "step  7000, loss 0.6617, acc 0.9325\n",
      "step  7500, loss 0.3120, acc 0.9325\n",
      "step  8000, loss 0.3888, acc 0.9325\n",
      "step  8500, loss 0.1861, acc 0.9351\n",
      "step  9000, loss 0.4032, acc 0.9342\n",
      "step  9500, loss 0.1506, acc 0.9374\n",
      "step 10000, loss 0.0687, acc 0.9366\n",
      "step 10500, loss 0.1012, acc 0.9370\n",
      "step 11000, loss 0.1640, acc 0.9391\n",
      "step 11500, loss 0.1264, acc 0.9399\n",
      "step 12000, loss 0.1681, acc 0.9439\n",
      "step 12500, loss 0.2316, acc 0.9421\n",
      "step 13000, loss 0.1549, acc 0.9422\n",
      "step 13500, loss 0.1760, acc 0.9425\n",
      "step 14000, loss 0.0373, acc 0.9455\n",
      "step 14500, loss 0.3457, acc 0.9460\n",
      "step 15000, loss 0.1957, acc 0.9464\n",
      "step 15500, loss 0.0622, acc 0.9494\n",
      "step 16000, loss 0.0702, acc 0.9467\n",
      "step 16500, loss 0.1093, acc 0.9510\n",
      "step 17000, loss 0.1298, acc 0.9487\n",
      "step 17500, loss 0.3511, acc 0.9516\n",
      "step 18000, loss 0.2645, acc 0.9520\n",
      "step 18500, loss 0.0961, acc 0.9535\n",
      "step 19000, loss 0.0680, acc 0.9528\n",
      "step 19500, loss 0.3924, acc 0.9534\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    # 隐藏层与输出层\n",
    "    hidden_output = tf.keras.layers.Dense(\n",
    "        128, activation=tf.nn.relu)(inputs)\n",
    "    \n",
    "    logits = tf.keras.layers.Dense(\n",
    "        10, activation=None)(hidden_output)\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=labels))\n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.比较使用高级API与使用低级API模型性能的差别，分析可能造成的原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 高级API是已经封装好的，使用起来简单，但对于某些特殊需求来说，不能细化控制，且一般情况下，使用高级API的模型性能比使用低级API的模型性能要好。\n",
    "#### 因为高级API中有许多参数已经被充分优化，许多细节已经被考虑到了，不会出现像之前在mnist分类中计算loss时为防止溢出（保证大于0）和过拟合时要加一个极小值这样的操作，加入极小值虽防止溢出和过拟合了，但是却相当于加入了噪声，从而导致对模型性能造成一定的影响，然而使用高级API却不会出现这样的情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
